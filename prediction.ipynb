{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf6a50a-d739-4cc1-8949-0ac5a1b8b2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6627b008-e9b0-45b0-b65a-402f3836c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "class simple_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simple_network, self).__init__()\n",
    "        self.conv0   = nn.Conv2d    (   1,  8,   kernel_size=(3, 3),     bias=False  , padding=1)   \n",
    "        self.max0    = nn.MaxPool2d (            kernel_size=(2,2)                   )   \n",
    "        self.conv1   = nn.Conv2d    (   8,  16,  kernel_size=(3, 3),     bias=False , padding=1 )   \n",
    "        self.max1    = nn.MaxPool2d (            kernel_size=(2,2)                   )   \n",
    "        self.flat    = nn.Flatten   (                                                )   \n",
    "        self.linear0 = nn.Linear    (            784,  784                           )\n",
    "        self.linear1 = nn.Linear    (            784,  10                            )\n",
    "        self.soft    = nn.Softmax   (            dim=1                               )\n",
    "        \n",
    "        self.debug=False\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        x=x.to(torch.device(\"cpu\"))\n",
    "        x_conv0      = self.conv0     (   x              )  \n",
    "        x_max0       = self.max0      (   x_conv0        )\n",
    "        x_conv1      = self.conv1     (   x_max0         )  \n",
    "        x_max1       = self.max1      (   x_conv1        )\n",
    "        x_flat       = self.flat      (   x_max1         )\n",
    "        x_linear0    = self.linear0   (   x_flat         )\n",
    "        x_linear1    = self.linear1   (   x_linear0      )\n",
    "        x_prob       = self.soft      (   x_linear1      )\n",
    "\n",
    "        # TODO: Fix Learning Rate and Gradient Descent Method, check batch size\n",
    "        if self.debug:\n",
    "            im=x[0][0]\n",
    "            print(f\"Input Image: {im[-4]}\\n\")\n",
    "            \n",
    "            print(f\"conv0 filters: {model.state_dict()['conv0.weight'][0][0]}\\n\")\n",
    "            \n",
    "            print(f\"x_conv0 : {x_conv0[0,0,:,:][-1]}\\n\")\n",
    "\n",
    "            print(f\"MaxPool0: {x_max0[0,0][-1]}\\n\")\n",
    "            \n",
    "            print(f\"conv1 filters: {model.state_dict()['conv1.weight'][0][0]}\\n\")\n",
    "            \n",
    "            print(f\"x_conv1 : {x_conv1[0,0,:,:][-1]}\\n\")\n",
    "\n",
    "            print(f\"MaxPool1: {x_max1[0,0][-1]}\\n\")\n",
    "            \n",
    "            print(f\"FC0 Weight: {model.state_dict()['linear0.weight'][0][:10]}\\n\")\n",
    "            \n",
    "            print(f\"FC0 Output: {x_linear0[0,:10]}\\n\")\n",
    "            \n",
    "            print(f\"FC1 Weight: {model.state_dict()['linear1.weight'][0][:10]}\\n\")\n",
    "            \n",
    "            print(f\"FC1 Output: {x_linear1[0,:10]}\\n\")\n",
    "            \n",
    "            print(f\"SoftMax Output: {x_prob}\\n\")\n",
    "            \n",
    "        return x_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e9e4b95-6397-4a9b-a4a7-82630d5a6825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "# Function for copying weights from basic model \n",
    "def copy_weights(name=\"weights/last.pkl\"):\n",
    "    # TODO: To copy filter weights\n",
    "    # name = \"weights/debug.pkl\"\n",
    "    print(f\"Loading weights from {name}\")\n",
    "    weight_file = open(name, \"rb\")\n",
    "    weights = pickle.load(weight_file)\n",
    "    print(f\"\\nLoading weights from {name} file\")\n",
    "    \n",
    "    fconv0   = weights[\"conv0\"]  \n",
    "    fconv1   = weights[\"conv1\"]  \n",
    "    flinear0 = weights[\"fc0_weights\"]\n",
    "    blinear0 = weights[\"fc0_biases\" ]\n",
    "    flinear1 = weights[\"fc1_weights\"]\n",
    "    blinear1 = weights[\"fc1_biases\" ]\n",
    "\n",
    "    debug=False\n",
    "    for i in range(fconv0.shape[0]):\n",
    "        model.state_dict()['conv0.weight'][i][0]     = torch.from_numpy(fconv0[i]).double()\n",
    "        if debug: print(f\"Weights for Conv0 Filter {i} are {model.state_dict()['conv0.weight'][i][0][0]}   \")\n",
    "\n",
    "    for i in range(fconv1.shape[0]):\n",
    "        for j in range(fconv1.shape[3]):\n",
    "            model.state_dict()['conv1.weight'][i,j,:,:]     = torch.from_numpy(fconv1[i,:,:,j]).double()\n",
    "        if debug: print(f\"Weights for Conv1 Filter {i} are {model.state_dict()['conv1.weight'][i][0][0]}   \")\n",
    "\n",
    "    for i in range(flinear0.shape[1]):\n",
    "        model.state_dict()['linear0.weight'][i]      = torch.from_numpy(flinear0[:,i]).double()\n",
    "        if debug and i<10: print(f\"Weights for Linear Filter {i} are {model.state_dict()['linear0.weight'][i][:5]} \")\n",
    "        \n",
    "    for i in range(blinear0.shape[0]):\n",
    "        model.state_dict()['linear0.bias'][i]        = torch.tensor(blinear0[i]).double()\n",
    "        if debug and i<10: print(f\"Weights for Linear Bias   {i} are {model.state_dict()['linear0.bias'][i]}   \")\n",
    "    \n",
    "    for i in range(flinear1.shape[1]):\n",
    "        model.state_dict()['linear1.weight'][i]      = torch.from_numpy(flinear1[:,i]).double()\n",
    "        if debug and i<10: print(f\"Weights for Linear Filter {i} are {model.state_dict()['linear1.weight'][i][:5]} \")\n",
    "        \n",
    "    for i in range(blinear1.shape[0]):\n",
    "        model.state_dict()['linear1.bias'][i]        = torch.tensor(blinear1[i]).double()\n",
    "        if debug and i<10: print(f\"Weights for Linear Bias   {i} are {model.state_dict()['linear1.bias'][i]}   \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c98cd4c0-cf2a-4edb-ab8a-3e1b0303c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# func to draw\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cded6c-fdbf-48c2-87f1-1bfb4d176190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abaf0251-030a-4f4c-845b-3e371c3507c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from weights/last.pkl\n",
      "\n",
      "Loading weights from weights/last.pkl file\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = simple_network()\n",
    "model.double()\n",
    "copy_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e6d4c95-9fd1-4708-b049-058980271ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load samples from MNIST test dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "data_folder = '../MNIST'\n",
    "tedt = datasets.MNIST(\n",
    "    root=data_folder,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "trdl = DataLoader(trdt, batch_size=5)\n",
    "\n",
    "trit = iter(trdl)\n",
    "x,y = next(trit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d810503-8d17-4ffe-b8ed-bd65a02f79f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw samples\n",
    "from torchvision.utils import make_grid\n",
    "show(make_grid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d6165a91-b47e-4cf0-a11c-a7a7d150e195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "x = x.double()\n",
    "pre = model(x)\n",
    "\n",
    "pre_arg = pre.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9c33a8f5-8d35-44e9-8144-aa9032560a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT: [5, 0, 4, 1, 9] \n",
      "Pre: [3, 0, 4, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "print(f'GT: {y.tolist()} \\nPre: {pre_arg.tolist()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch0",
   "language": "python",
   "name": "torch0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
